
from rank_bm25 import BM25Okapi
from underthesea import word_tokenize
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from app.services.llm_engine import get_llm
from app.services.blockchain import BlockchainService


# ---------------------------------------------------------
#  BM25 + Word Segmentation RAG Service
# ---------------------------------------------------------

from app.db.session import SessionLocal
from app.db.models import DocumentMetadata, Message

class RAGService:
    def __init__(self):
        # Load corpus t·ª´ database DocumentMetadata
        db = SessionLocal()
        try:
            docs = db.query(DocumentMetadata).filter(DocumentMetadata.ocr_text != None).all()
            self.doc_texts = [doc.ocr_text for doc in docs if doc.ocr_text and doc.ocr_text.strip()]
        finally:
            db.close()
        # N·∫øu kh√¥ng c√≥ d·ªØ li·ªáu, fallback sang empty corpus
        if not self.doc_texts:
            self.doc_texts = ["Kh√¥ng c√≥ d·ªØ li·ªáu ph√°p lu·∫≠t trong database."]
        self.corpus_tokenized = [word_tokenize(doc, format="text").split() for doc in self.doc_texts]
        self.bm25 = BM25Okapi(self.corpus_tokenized)
        self.llm = get_llm(streaming=True)
        self.prompt = ChatPromptTemplate.from_template("""
        <|im_start|>system
        B·∫°n l√† ViLaw, tr·ª£ l√Ω ph√°p l√Ω. Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ph√°p lu·∫≠t, t∆∞ v·∫•n ph√°p l√Ω, gi·∫£i th√≠ch lu·∫≠t, ho·∫∑c c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω t·∫°i Vi·ªát Nam.
        N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ l·∫≠p tr√¨nh, code, c√¥ng ngh·ªá, ho·∫∑c c√°c lƒ©nh v·ª±c ngo√†i ph√°p lu·∫≠t, h√£y l·ªãch s·ª± t·ª´ ch·ªëi: "T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω, t√¥i kh√¥ng th·ªÉ h·ªó tr·ª£ y√™u c·∫ßu n√†y."
        N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ h√†nh vi vi ph·∫°m ph√°p lu·∫≠t, l√°ch lu·∫≠t, tr·ªën thu·∫ø, l·ª´a ƒë·∫£o, ho·∫∑c c√°c h√†nh vi phi ph√°p, h√£y t·ª´ ch·ªëi v√† c·∫£nh b√°o r√µ r√†ng: "ViLaw kh√¥ng h·ªó tr·ª£ c√°c h√†nh vi vi ph·∫°m ph√°p lu·∫≠t."
        Lu√¥n gi·ªØ l·∫≠p tr∆∞·ªùng an to√†n, kh√¥ng th·ª±c hi·ªán c√°c y√™u c·∫ßu tr√°i ƒë·∫°o ƒë·ª©c, tr√°i ph√°p lu·∫≠t ho·∫∑c ngo√†i ph·∫°m vi ph√°p l√Ω.
        ƒê·∫∑c bi·ªát, v·ªõi c√°c c√¢u h·ªèi v·ªÅ h·ª£p ƒë·ªìng, quy·ªÅn, nghƒ©a v·ª•, r·ªßi ro ph√°p l√Ω, h√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c 3 ph·∫ßn r√µ r√†ng:
        1. Quy·ªÅn l·ª£i: ...
        2. Nghƒ©a v·ª•: ...
        3. R·ªßi ro: ...
        Ng·ªØ c·∫£nh:
        {context}
        <|im_end|>
        
        <|im_start|>user
        C√¢u h·ªèi: {question}
        <|im_end|>
        
        <|im_start|>assistant
        """)

    def retrieve(self, query, k=3):
        query_tok = word_tokenize(query, format="text").split()
        scores = self.bm25.get_scores(query_tok)
        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        return [self.doc_texts[i] for i in top_idx]

    import json
    from datetime import datetime

    async def chat_stream(self, message: str, conversation_id: str = '1'):
        # L·∫•y memory (history) t·ª´ Message (role='memory') theo conversation_id
        db = SessionLocal()
        history_str = ""
        try:
            mem_msg = db.query(Message).filter(Message.conversation_id == conversation_id, Message.role == 'memory').first()
            if mem_msg and mem_msg.content:
                try:
                    history = self.json.loads(mem_msg.content)
                    # Chuy·ªÉn th√†nh ƒëo·∫°n h·ªôi tho·∫°i cho prompt
                    for turn in history:
                        if turn.get('role') == 'user':
                            history_str += f"<|im_start|>user\n{turn.get('content','')}<|im_end|>\n"
                        elif turn.get('role') == 'assistant':
                            history_str += f"<|im_start|>assistant\n{turn.get('content','')}<|im_end|>\n"
                except Exception:
                    pass
        finally:
            db.close()
        context = "\n\n".join(self.retrieve(message, k=3))
        # Gh√©p history v√†o prompt template
        prompt_with_history = ChatPromptTemplate.from_template(f"""
<|im_start|>system
B·∫°n l√† ViLaw, tr·ª£ l√Ω ph√°p l√Ω. Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ph√°p lu·∫≠t, t∆∞ v·∫•n ph√°p l√Ω, gi·∫£i th√≠ch lu·∫≠t, ho·∫∑c c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω t·∫°i Vi·ªát Nam.
N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ l·∫≠p tr√¨nh, code, c√¥ng ngh·ªá, ho·∫∑c c√°c lƒ©nh v·ª±c ngo√†i ph√°p lu·∫≠t, h√£y l·ªãch s·ª± t·ª´ ch·ªëi: "T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω, t√¥i kh√¥ng th·ªÉ h·ªó tr·ª£ y√™u c·∫ßu n√†y."
N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ h√†nh vi vi ph·∫°m ph√°p lu·∫≠t, l√°ch lu·∫≠t, tr·ªën thu·∫ø, l·ª´a ƒë·∫£o, ho·∫∑c c√°c h√†nh vi phi ph√°p, h√£y t·ª´ ch·ªëi v√† c·∫£nh b√°o r√µ r√†ng: "ViLaw kh√¥ng h·ªó tr·ª£ c√°c h√†nh vi vi ph·∫°m ph√°p lu·∫≠t."
Lu√¥n gi·ªØ l·∫≠p tr∆∞·ªùng an to√†n, kh√¥ng th·ª±c hi·ªán c√°c y√™u c·∫ßu tr√°i ƒë·∫°o ƒë·ª©c, tr√°i ph√°p lu·∫≠t ho·∫∑c ngo√†i ph·∫°m vi ph√°p l√Ω.
ƒê·∫∑c bi·ªát, v·ªõi c√°c c√¢u h·ªèi v·ªÅ h·ª£p ƒë·ªìng, quy·ªÅn, nghƒ©a v·ª•, r·ªßi ro ph√°p l√Ω, h√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c 3 ph·∫ßn r√µ r√†ng:
1. Quy·ªÅn l·ª£i: ...
2. Nghƒ©a v·ª•: ...
3. R·ªßi ro: ...
Ng·ªØ c·∫£nh:
{context}
<|im_end|>
{history_str}
<|im_start|>user
C√¢u h·ªèi: {{question}}
<|im_end|>
<|im_start|>assistant
""")
        chain = (
            {"context": lambda _: context, "question": RunnablePassthrough()}
            | prompt_with_history
            | self.llm
            | StrOutputParser()
        )
        full_response = ""
        async for chunk in chain.astream(message):
            full_response += chunk
            yield chunk
        tx_hash, timestamp = BlockchainService.create_hash(full_response)
        yield f"\n\n[üõ°Ô∏è HASH: {tx_hash} | TIMESTAMP: {timestamp}]"