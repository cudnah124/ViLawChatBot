import json
from datetime import datetime
from rank_bm25 import BM25Okapi
from underthesea import word_tokenize
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from app.services.llm_engine import get_llm
from app.services.blockchain import BlockchainService
from app.db.session import SessionLocal
from app.db.models import LawChunk, ChatHistory

class RAGService:
    _instance = None
    _bm25 = None
    _doc_texts = None
    _llm = None
    
    # ƒê·ªãnh nghƒ©a System Prompt l√† h·∫±ng s·ªë ƒë·ªÉ d·ªÖ qu·∫£n l√Ω, tr√°nh vi·∫øt l·∫∑p l·∫°i
    SYSTEM_PROMPT = """
    <|im_start|>system
    B·∫°n l√† ViLaw, tr·ª£ l√Ω ph√°p l√Ω. Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ph√°p lu·∫≠t t·∫°i Vi·ªát Nam.
    N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ l·∫≠p tr√¨nh, code, c√¥ng ngh·ªá, ho·∫∑c phi ph√°p, h√£y t·ª´ ch·ªëi l·ªãch s·ª±.
    ƒê·∫∑c bi·ªát, v·ªõi c√¢u h·ªèi v·ªÅ h·ª£p ƒë·ªìng/quy·ªÅn/nghƒ©a v·ª•, tr·∫£ l·ªùi theo c·∫•u tr√∫c 3 ph·∫ßn:
    1. Quy·ªÅn l·ª£i: ...
    2. Nghƒ©a v·ª•: ...
    3. R·ªßi ro: ...
    Ng·ªØ c·∫£nh:
    {context}
    <|im_end|>
    """

    def __new__(cls):
        # Singleton Pattern: Ch·ªâ kh·ªüi t·∫°o 1 l·∫ßn duy nh·∫•t
        if cls._instance is None:
            cls._instance = super(RAGService, cls).__new__(cls)
            # cls._init_resources()
        return cls._instance

    @classmethod
    def _init_resources(cls):
        """H√†m n√†y ch·ªâ ch·∫°y 1 l·∫ßn khi server kh·ªüi ƒë·ªông"""
        print("--- RAGService: Initializing Resources... ---")


        db = SessionLocal()
        try:
            laws = db.query(LawChunk).filter(LawChunk.content != None).all()
            cls._doc_texts = [law.content for law in laws if law.content and law.content.strip()]
        finally:
            db.close()

        if not cls._doc_texts:
            cls._doc_texts = ["Kh√¥ng c√≥ d·ªØ li·ªáu ph√°p lu·∫≠t trong database."]

        # Tokenize & Build BM25
        # L∆∞u √Ω: corpus_tokenized kh√¥ng c·∫ßn l∆∞u v√†o class attribute n·∫øu ch·ªâ d√πng ƒë·ªÉ build bm25
        corpus_tokenized = [word_tokenize(doc, format="text").split() for doc in cls._doc_texts]
        cls._bm25 = BM25Okapi(corpus_tokenized)
        
        # Init LLM
        cls._llm = get_llm(streaming=True)
        print("--- RAGService: Ready ---")

    def retrieve(self, query, k=3):
        # S·ª≠a l·ªói: D√πng self._bm25 thay v√¨ self.bm25
        query_tok = word_tokenize(query, format="text").split()
        scores = self._bm25.get_scores(query_tok)
        top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:k]
        return [self._doc_texts[i] for i in top_idx]

    def _create_prompt(self, history_str):
        """H√†m helper ƒë·ªÉ gh√©p prompt ƒë·ªông"""
        full_template = f"""
{self.SYSTEM_PROMPT}
{history_str}
<|im_start|>user
C√¢u h·ªèi: {{question}}
<|im_end|>
<|im_start|>assistant
"""
        return ChatPromptTemplate.from_template(full_template)

    @classmethod
    def refresh_knowledge(cls):
        """Public method to refresh/reload RAG resources (rebuild BM25, reload chunks)."""
        try:
            cls._init_resources()
            print("RAGService: Knowledge refreshed.")
        except Exception as e:
            print(f"RAGService.refresh_knowledge error: {e}")

    async def chat_stream(self, message: str, conversation_id: str = '1', db=None):
        # Logic qu·∫£n l√Ω DB Session
        close_db = False
        if db is None:
            db = SessionLocal()
            close_db = True
            
        history_str = ""
        try:
            # Try to use a Message model if it exists; otherwise skip memory loading
            Message = None
            try:
                from app.db.models import Message as _Message
                Message = _Message
            except Exception:
                Message = None

            if Message:
                mem_msg = db.query(Message).filter(Message.conversation_id == conversation_id, Message.role == 'memory').first()
                if mem_msg and getattr(mem_msg, 'content', None):
                    try:
                        history = json.loads(mem_msg.content)
                        for turn in history:
                            role = turn.get('role')
                            content = turn.get('content', '')
                            if role in ['user', 'assistant']:
                                history_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
                    except Exception:
                        pass
        finally:
            if close_db:
                db.close()

        # Ensure resources are initialized (BM25, docs, LLM)
        if not getattr(self, '_bm25', None):
            try:
                type(self)._init_resources()
            except Exception as e:
                print(f"RAGService: failed to init resources: {e}")

        # 1. Retrieve Context
        context = "\n\n".join(self.retrieve(message, k=3))
        
        # 2. Create Chain (T√°i s·ª≠ d·ª•ng prompt template g·ªçn g√†ng h∆°n)
        prompt_template = self._create_prompt(history_str)
        
        chain = (
            {"context": lambda _: context, "question": RunnablePassthrough()}
            | prompt_template
            | self._llm
            | StrOutputParser()
        )

        # 3. Streaming & Blockchain
        full_response = ""
        async for chunk in chain.astream(message):
            full_response += chunk
            yield chunk
            
        tx_hash, timestamp = BlockchainService.create_hash(full_response)
        yield f"\n\n[üõ°Ô∏è HASH: {tx_hash} | TIMESTAMP: {timestamp}]"