import os
import hashlib
from datetime import datetime
from typing import List

# 1. Import th∆∞ vi·ªán c·ªßa b·∫°n
import torch
from transformers import AutoTokenizer, AutoModel, RobertaModel
from langchain_core.embeddings import Embeddings # Class cha ƒë·ªÉ custom
from langchain_pinecone import PineconeVectorStore
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

from app.core.config import settings
from app.services.llm_engine import get_llm
from app.services.blockchain import BlockchainService

# ---------------------------------------------------------
# 2. ƒê·ªäNH NGHƒ®A CLASS CUSTOM EMBEDDING (D√πng code c·ªßa b·∫°n)
# ---------------------------------------------------------
class VietnameseSBERTEmbeddings(Embeddings):
    def _mean_pooling(self, model_output, attention_mask):
        """
        Mean Pooling - l·∫•y embedding trung b√¨nh cho m·ªói c√¢u
        """
        token_embeddings = model_output[0]  # First element of output contains token embeddings
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

    def __init__(self, model_name: str = "keepitreal/vietnamese-sbert"):
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            # Thay AutoModel b·∫±ng RobertaModel
            self.model = RobertaModel.from_pretrained(model_name)
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model.to(self.device)
            self.model.eval()
        except Exception as e:
            raise RuntimeError(f"Failed to load model: {e}")

    def _compute_embedding(self, texts: List[str]) -> List[List[float]]:
        try:
            # Tokenize v·ªõi max_length ƒë·ªÉ tr√°nh qu√° d√†i
            encoded_input = self.tokenizer(
                texts, 
                padding=True, 
                truncation=True, 
                max_length=512,  # Th√™m gi·ªõi h·∫°n
                return_tensors='pt'
            )
            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}

            with torch.no_grad():
                model_output = self.model(**encoded_input)

            sentence_embeddings = self._mean_pooling(
                model_output, 
                encoded_input['attention_mask']
            )
            
            return sentence_embeddings.cpu().tolist()
        except Exception as e:
            raise RuntimeError(f"Embedding computation failed: {e}")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """D√πng cho list vƒÉn b·∫£n (Khi n·∫°p data)"""
        return self._compute_embedding(texts)

    def embed_query(self, text: str) -> List[float]:
        """D√πng cho 1 c√¢u h·ªèi (Khi chat)"""
        return self._compute_embedding([text])[0]

# ---------------------------------------------------------
# 3. S·ª¨ D·ª§NG CLASS M·ªöI TRONG RAG SERVICE
# ---------------------------------------------------------
class RAGService:
    def __init__(self):
        # Thay v√¨ d√πng HuggingFaceEmbeddings c√≥ s·∫µn, ta d√πng Class t·ª± vi·∫øt
        self.embedding_model = VietnameseSBERTEmbeddings()

        # K·∫øt n·ªëi Pinecone
        self.vector_db = PineconeVectorStore.from_existing_index(
            index_name=settings.PINECONE_INDEX_NAME,
            embedding=self.embedding_model
        )
        
        self.retriever = self.vector_db.as_retriever(search_kwargs={"k": 3})
        self.llm = get_llm(streaming=True)
        
        self.prompt = ChatPromptTemplate.from_template("""
        <|im_start|>system
        B·∫°n l√† ViLaw, tr·ª£ l√Ω ph√°p l√Ω. Ch·ªâ tr·∫£ l·ªùi c√°c c√¢u h·ªèi li√™n quan ƒë·∫øn ph√°p lu·∫≠t, t∆∞ v·∫•n ph√°p l√Ω, gi·∫£i th√≠ch lu·∫≠t, ho·∫∑c c√°c v·∫•n ƒë·ªÅ ph√°p l√Ω t·∫°i Vi·ªát Nam.
        N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ l·∫≠p tr√¨nh, code, c√¥ng ngh·ªá, ho·∫∑c c√°c lƒ©nh v·ª±c ngo√†i ph√°p lu·∫≠t, h√£y l·ªãch s·ª± t·ª´ ch·ªëi: "T√¥i l√† tr·ª£ l√Ω ph√°p l√Ω, t√¥i kh√¥ng th·ªÉ h·ªó tr·ª£ y√™u c·∫ßu n√†y."
        N·∫øu ng∆∞·ªùi d√πng h·ªèi v·ªÅ h√†nh vi vi ph·∫°m ph√°p lu·∫≠t, l√°ch lu·∫≠t, tr·ªën thu·∫ø, l·ª´a ƒë·∫£o, ho·∫∑c c√°c h√†nh vi phi ph√°p, h√£y t·ª´ ch·ªëi v√† c·∫£nh b√°o r√µ r√†ng: "ViLaw kh√¥ng h·ªó tr·ª£ c√°c h√†nh vi vi ph·∫°m ph√°p lu·∫≠t."
        Lu√¥n gi·ªØ l·∫≠p tr∆∞·ªùng an to√†n, kh√¥ng th·ª±c hi·ªán c√°c y√™u c·∫ßu tr√°i ƒë·∫°o ƒë·ª©c, tr√°i ph√°p lu·∫≠t ho·∫∑c ngo√†i ph·∫°m vi ph√°p l√Ω.
        ƒê·∫∑c bi·ªát, v·ªõi c√°c c√¢u h·ªèi v·ªÅ h·ª£p ƒë·ªìng, quy·ªÅn, nghƒ©a v·ª•, r·ªßi ro ph√°p l√Ω, h√£y tr·∫£ l·ªùi theo c·∫•u tr√∫c 3 ph·∫ßn r√µ r√†ng:
        1. Quy·ªÅn l·ª£i: ...
        2. Nghƒ©a v·ª•: ...
        3. R·ªßi ro: ...
        Ng·ªØ c·∫£nh:
        {context}
        <|im_end|>
        
        <|im_start|>user
        C√¢u h·ªèi: {question}
        <|im_end|>
        
        <|im_start|>assistant
        """)

    def format_docs(self, docs):
        return "\n\n".join(doc.page_content for doc in docs)

    async def chat_stream(self, question: str):
        chain = (
            {"context": self.retriever | self.format_docs, "question": RunnablePassthrough()}
            | self.prompt
            | self.llm
            | StrOutputParser()
        )

        full_response = ""
        async for chunk in chain.astream(question):
            full_response += chunk
            yield chunk

        tx_hash, timestamp = BlockchainService.create_hash(full_response)
        yield f"\n\n[üõ°Ô∏è HASH: {tx_hash} | TIMESTAMP: {timestamp}]"